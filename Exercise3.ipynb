{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.decomposition\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE CHANGE THE VALUE OF DATASET TO CHANGE THE DATASET (0 = digits, 1 = 20 newsgroups)\n",
    "dataset = 0\n",
    "\n",
    "if dataset == 0:\n",
    "    digits = sklearn.datasets.load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "elif dataset == 2:\n",
    "    newsgroups = fetch_20newsgroups(subset='all', \n",
    "                                remove=('headers', 'footers', 'quotes'),\n",
    "                                random_state=42)\n",
    "\n",
    "    # Convert text to TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(newsgroups.data)\n",
    "    y = newsgroups.target\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(X, y, clf, cv=10):\n",
    "    # Initialize cross-validation\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Split data into train and test folds\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        \n",
    "        # Fit the classifier on the training fold\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict on the test fold\n",
    "        y_pred = clf.predict(X_test_fold)\n",
    "        \n",
    "        # Compute accuracy for the fold\n",
    "        scores.append(accuracy_score(y_test_fold, y_pred))\n",
    "    \n",
    "    # Return the mean accuracy across all folds\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"Mean Cross-Validation Accuracy: {mean_score}\")\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'pca__n_components': 38}\n",
      "Best Cross-Validation Accuracy: 0.9777333333333331\n",
      "Test Accuracy with Best Model: 0.9777777777777777\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.9785142857142857\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': np.arange(2, 60, 2),  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with PCA and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and accuracy\n",
    "best_params_pca = grid_search.best_params_\n",
    "best_accuracy_pca = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_pca}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_pca}\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "best_model_pca = grid_search.best_estimator_\n",
    "test_accuracy_pca = best_model_pca.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_pca}\")\n",
    "\n",
    "# Now, use the compute_test function to print the mean accuracy during cross-validation\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_pca = compute_test(X_train, y_train, best_model_pca, cv=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'manhattan', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'lda__n_components': 8}\n",
      "Best Cross-Validation Accuracy: 0.9625777777777778\n",
      "Test Accuracy with Best Model: 0.9611111111111111\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.959415873015873\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "\n",
    "if dataset == 0:\n",
    "    n_components = np.arange(1, 9, 1)\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(1, 19, 1)\n",
    "\n",
    "param_grid = {\n",
    "    'lda__n_components': n_components,  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with LDA and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('lda', LDA()),  # LDA step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and accuracy\n",
    "best_params_lda = grid_search.best_params_\n",
    "best_accuracy_lda = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_lda}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_lda}\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "best_model_lda = grid_search.best_estimator_\n",
    "test_accuracy_lda = best_model_lda.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_lda}\")\n",
    "\n",
    "# Now, use the compute_test function to print the mean accuracy during cross-validation\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_lda = compute_test(X_train, y_train, best_model_lda, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 24}\n",
      "Best Cross-Validation Accuracy: 0.9777333333333331\n",
      "Test Accuracy with Best Model: 0.975925925925926\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.9729460317460319\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'svd__n_components': np.arange(2, 60, 2),  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with SVD and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('svd', TruncatedSVD()),  # SVD step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and accuracy\n",
    "best_params_svd = grid_search.best_params_\n",
    "best_accuracy_svd = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_svd}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_svd}\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "best_model_svd = grid_search.best_estimator_\n",
    "test_accuracy_svd = best_model_svd.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_svd}\")\n",
    "\n",
    "# Now, use the compute_test function to print the mean accuracy during cross-validation\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_svd = compute_test(X_train, y_train, best_model_svd, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Method: PCA\n",
      "Best Parameters for PCA: {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'pca__n_components': 38}\n",
      "Best Cross-Validation Accuracy for PCA: 0.9777333333333331\n",
      "Test Accuracy for PCA: 0.9777777777777777\n",
      "Cross-Validation Accuracy for PCA: 0.9785142857142857\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'PCA': {\n",
    "        'best_params': best_params_pca,\n",
    "        'best_accuracy': best_accuracy_pca,\n",
    "        'test_accuracy': test_accuracy_pca,\n",
    "        'cv_accuracy': result_pca\n",
    "    },\n",
    "    'SVD': {\n",
    "        'best_params': best_params_svd,\n",
    "        'best_accuracy': best_accuracy_svd,\n",
    "        'test_accuracy': test_accuracy_svd,\n",
    "        'cv_accuracy': result_svd\n",
    "    },\n",
    "    'LDA': {\n",
    "        'best_params': best_params_lda,\n",
    "        'best_accuracy': best_accuracy_lda,\n",
    "        'test_accuracy': test_accuracy_lda,\n",
    "        'cv_accuracy': result_lda\n",
    "    }\n",
    "}\n",
    "\n",
    "# Find the method with the highest test accuracy\n",
    "best_method = max(results, key=lambda x: results[x]['test_accuracy'])\n",
    "\n",
    "# Print out the best method and comparison results\n",
    "print(f\"\\nBest Method: {best_method}\")\n",
    "print(f\"Best Parameters for {best_method}: {results[best_method]['best_params']}\")\n",
    "print(f\"Best Cross-Validation Accuracy for {best_method}: {results[best_method]['best_accuracy']}\")\n",
    "print(f\"Test Accuracy for {best_method}: {results[best_method]['test_accuracy']}\")\n",
    "print(f\"Cross-Validation Accuracy for {best_method}: {results[best_method]['cv_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for MLP: {'mlp__hidden_layer_sizes': (150, 150), 'mlp__learning_rate_init': 0.02, 'mlp__max_iter': 100, 'pca__n_components': 54}\n",
      "Best Cross-Validation Accuracy for MLP: 0.9753206349206348\n",
      "Test Accuracy for MLP: 0.975925925925926\n",
      "Mean Cross-Validation Accuracy: 0.9705650793650793\n",
      "Mean Cross-Validation Accuracy for MLP: 0.9705650793650793\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for MLP\n",
    "param_grid_mlp = {\n",
    "    'pca__n_components': np.arange(2, 60, 2),  # Number of components to test for PCA\n",
    "    'mlp__hidden_layer_sizes': [(100, 100), (150, 150), (50, 50)],  # Hidden layer configurations\n",
    "    'mlp__learning_rate_init': [0.001, 0.01, 0.02],  # Learning rates\n",
    "    'mlp__max_iter': [10, 50, 100]  # Iterations for training\n",
    "}\n",
    "\n",
    "# Create a pipeline with PCA and MLPClassifier\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('mlp', MLPClassifier(activation='relu', solver='sgd', learning_rate='constant'))  # MLP step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV for MLP\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    pipeline_mlp,\n",
    "    param_grid_mlp,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search for MLP\n",
    "grid_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and accuracy for MLP\n",
    "best_params_mlp = grid_search_mlp.best_params_\n",
    "best_accuracy_mlp = grid_search_mlp.best_score_\n",
    "\n",
    "print(f\"Best Parameters for MLP: {best_params_mlp}\")\n",
    "print(f\"Best Cross-Validation Accuracy for MLP: {best_accuracy_mlp}\")\n",
    "\n",
    "# Evaluate the best model on the test data for MLP\n",
    "best_model_mlp = grid_search_mlp.best_estimator_\n",
    "test_accuracy_mlp = best_model_mlp.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for MLP: {test_accuracy_mlp}\")\n",
    "\n",
    "# Perform compute_test to evaluate cross-validation performance\n",
    "result_mlp = compute_test(X_train, y_train, best_model_mlp, cv=10)\n",
    "print(f\"Mean Cross-Validation Accuracy for MLP: {result_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Bagging: {'bagging__estimator__max_depth': 10, 'bagging__n_estimators': 100, 'pca__n_components': 20}\n",
      "Best Cross-Validation Accuracy for Bagging: 0.9021532915955227\n",
      "Best Parameters for AdaBoost: {'adaboost__learning_rate': 0.01, 'adaboost__n_estimators': 50, 'pca__n_components': 20}\n",
      "Best Cross-Validation Accuracy for AdaBoost: 0.5894770125845823\n",
      "Best Parameters for Gradient Boosting: {'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'gb__n_estimators': 100, 'pca__n_components': 30}\n",
      "Best Cross-Validation Accuracy for Gradient Boosting: 0.9204357174476696\n",
      "Test Accuracy for Bagging: 0.8981481481481481\n",
      "Test Accuracy for AdaBoost: 0.6055555555555555\n",
      "Test Accuracy for Gradient Boosting: 0.924074074074074\n",
      "Mean Cross-Validation Accuracy: 0.9013216973376336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Accuracy: 0.5560899260102448\n",
      "Mean Cross-Validation Accuracy: 0.9085088218554354\n",
      "Mean Cross-Validation Accuracy for Bagging: 0.9013216973376336\n",
      "Mean Cross-Validation Accuracy for AdaBoost: 0.5560899260102448\n",
      "Mean Cross-Validation Accuracy for Gradient Boosting: 0.9085088218554354\n"
     ]
    }
   ],
   "source": [
    "param_grid_bagging = {\n",
    "    'pca__n_components': [2, 10, 20, 30],  # PCA components to test\n",
    "    'bagging__n_estimators': [10, 50, 100],  # Number of base learners\n",
    "    'bagging__estimator__max_depth': [5, 10]  # Max depth for base estimator (DecisionTree)\n",
    "}\n",
    "\n",
    "# AdaBoost Classifier\n",
    "param_grid_adaboost = {\n",
    "    'pca__n_components': [2, 10, 20, 30],  # PCA components to test\n",
    "    'adaboost__n_estimators': [50, 100],  # Number of base learners\n",
    "    'adaboost__learning_rate': [0.01, 0.1]  # Learning rate for AdaBoost\n",
    "}\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "param_grid_gb = {\n",
    "    'pca__n_components': [2, 10, 20, 30],  # PCA components to test\n",
    "    'gb__n_estimators': [50, 100],  # Number of base learners\n",
    "    'gb__learning_rate': [0.01, 0.1],  # Learning rate for Gradient Boosting\n",
    "    'gb__max_depth': [3, 5]  # Max depth for trees in Gradient Boosting\n",
    "}\n",
    "\n",
    "# Create pipelines for each ensemble method with PCA\n",
    "pipeline_bagging = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('bagging', BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42))  # Bagging step\n",
    "])\n",
    "\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('adaboost', AdaBoostClassifier(random_state=42))  # AdaBoost step\n",
    "])\n",
    "\n",
    "pipeline_gb = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))  # Gradient Boosting step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV for each ensemble method\n",
    "grid_search_bagging = GridSearchCV(pipeline_bagging, param_grid_bagging, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_adaboost = GridSearchCV(pipeline_adaboost, param_grid_adaboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search for each ensemble method\n",
    "grid_search_bagging.fit(X_train, y_train)\n",
    "grid_search_adaboost.fit(X_train, y_train)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best parameters and accuracy for Bagging\n",
    "best_params_bagging = grid_search_bagging.best_params_\n",
    "best_accuracy_bagging = grid_search_bagging.best_score_\n",
    "print(f\"Best Parameters for Bagging: {best_params_bagging}\")\n",
    "print(f\"Best Cross-Validation Accuracy for Bagging: {best_accuracy_bagging}\")\n",
    "\n",
    "# Retrieve best parameters and accuracy for AdaBoost\n",
    "best_params_adaboost = grid_search_adaboost.best_params_\n",
    "best_accuracy_adaboost = grid_search_adaboost.best_score_\n",
    "print(f\"Best Parameters for AdaBoost: {best_params_adaboost}\")\n",
    "print(f\"Best Cross-Validation Accuracy for AdaBoost: {best_accuracy_adaboost}\")\n",
    "\n",
    "# Retrieve best parameters and accuracy for Gradient Boosting\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "best_accuracy_gb = grid_search_gb.best_score_\n",
    "print(f\"Best Parameters for Gradient Boosting: {best_params_gb}\")\n",
    "print(f\"Best Cross-Validation Accuracy for Gradient Boosting: {best_accuracy_gb}\")\n",
    "\n",
    "# Evaluate the best model for Bagging on test data\n",
    "best_model_bagging = grid_search_bagging.best_estimator_\n",
    "test_accuracy_bagging = best_model_bagging.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for Bagging: {test_accuracy_bagging}\")\n",
    "\n",
    "# Evaluate the best model for AdaBoost on test data\n",
    "best_model_adaboost = grid_search_adaboost.best_estimator_\n",
    "test_accuracy_adaboost = best_model_adaboost.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for AdaBoost: {test_accuracy_adaboost}\")\n",
    "\n",
    "# Evaluate the best model for Gradient Boosting on test data\n",
    "best_model_gb = grid_search_gb.best_estimator_\n",
    "test_accuracy_gb = best_model_gb.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for Gradient Boosting: {test_accuracy_gb}\")\n",
    "\n",
    "# Perform compute_test for each ensemble method\n",
    "result_bagging = compute_test(X_train, y_train, best_model_bagging, cv=5)\n",
    "result_adaboost = compute_test(X_train, y_train, best_model_adaboost, cv=5)\n",
    "result_gb = compute_test(X_train, y_train, best_model_gb, cv=5)\n",
    "\n",
    "print(f\"Mean Cross-Validation Accuracy for Bagging: {result_bagging}\")\n",
    "print(f\"Mean Cross-Validation Accuracy for AdaBoost: {result_adaboost}\")\n",
    "print(f\"Mean Cross-Validation Accuracy for Gradient Boosting: {result_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Ensemble Method: Gradient Boosting\n",
      "Best Parameters: {'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'gb__n_estimators': 100, 'pca__n_components': 30}\n",
      "Test Accuracy: 0.924074074074074\n",
      "Mean Cross-Validation Accuracy: 0.9084993359893758\n",
      "Mean Cross-Validation Accuracy for Best Model: 0.9084993359893758\n"
     ]
    }
   ],
   "source": [
    "# Store the best test accuracies and models\n",
    "best_model_info = {\n",
    "    \"Bagging\": {\"accuracy\": test_accuracy_bagging, \"model\": best_model_bagging, \"params\": best_params_bagging},\n",
    "    \"AdaBoost\": {\"accuracy\": test_accuracy_adaboost, \"model\": best_model_adaboost, \"params\": best_params_adaboost},\n",
    "    \"Gradient Boosting\": {\"accuracy\": test_accuracy_gb, \"model\": best_model_gb, \"params\": best_params_gb}\n",
    "}\n",
    "\n",
    "# Compare the test accuracies and print the best model\n",
    "best_model_name = max(best_model_info, key=lambda x: best_model_info[x][\"accuracy\"])\n",
    "best_model_accuracy = best_model_info[best_model_name][\"accuracy\"]\n",
    "best_model_params = best_model_info[best_model_name][\"params\"]\n",
    "\n",
    "print(f\"\\nBest Ensemble Method: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_model_params}\")\n",
    "print(f\"Test Accuracy: {best_model_accuracy}\")\n",
    "\n",
    "# Perform compute_test on the best model\n",
    "best_model = best_model_info[best_model_name][\"model\"]\n",
    "result_best_model = compute_test(X_train, y_train, best_model, cv=5)\n",
    "print(f\"Mean Cross-Validation Accuracy for Best Model: {result_best_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
