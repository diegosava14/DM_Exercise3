{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.decomposition\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE CHANGE THE VALUE OF DATASET TO CHANGE THE DATASET (0 = digits, 1 = 20 newsgroups)\n",
    "dataset = 1\n",
    "\n",
    "if dataset == 0:\n",
    "    digits = sklearn.datasets.load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "\n",
    "elif dataset == 1:\n",
    "    iris = load_iris()\n",
    "    X = iris.data  \n",
    "    y = iris.target  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(X, y, clf, cv=10):\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        y_pred = clf.predict(X_test_fold)\n",
    "        \n",
    "        scores.append(accuracy_score(y_test_fold, y_pred))\n",
    "    \n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"Mean Cross-Validation Accuracy: {mean_score}\")\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 14, 'knn__weights': 'uniform', 'pca__n_components': 4}\n",
      "Best Cross-Validation Accuracy: 0.9445454545454547\n",
      "Test Accuracy with Best Model: 1.0\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.9236363636363636\n"
     ]
    }
   ],
   "source": [
    "if dataset == 0:\n",
    "    n_components = np.arange(2, 60, 2)\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(2, 5, 1)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'pca__n_components': n_components,  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with PCA and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_pca = grid_search.best_params_\n",
    "best_accuracy_pca = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_pca}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_pca}\")\n",
    "\n",
    "best_model_pca = grid_search.best_estimator_\n",
    "test_accuracy_pca = best_model_pca.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_pca}\")\n",
    "\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_pca = compute_test(X_train, y_train, best_model_pca, cv=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 6, 'knn__weights': 'uniform', 'lda__n_components': 2}\n",
      "Best Cross-Validation Accuracy: 0.9727272727272727\n",
      "Test Accuracy with Best Model: 1.0\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.9718181818181819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 471, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 408, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1303, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 295, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 614, in fit\n",
      "    raise ValueError(\n",
      "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.92545455        nan        nan 0.92545455        nan        nan\n",
      " 0.91636364        nan        nan 0.92545455        nan        nan\n",
      " 0.92636364        nan        nan 0.92545455        nan        nan\n",
      " 0.93545455        nan        nan 0.92545455        nan        nan\n",
      " 0.96363636        nan        nan 0.94363636        nan        nan\n",
      " 0.97272727        nan        nan 0.94363636        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.96363636        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.96363636        nan        nan 0.94454545        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.96363636        nan        nan 0.94454545        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.92545455        nan        nan 0.92545455        nan        nan\n",
      " 0.91636364        nan        nan 0.92545455        nan        nan\n",
      " 0.92636364        nan        nan 0.92545455        nan        nan\n",
      " 0.92636364        nan        nan 0.92545455        nan        nan\n",
      " 0.96363636        nan        nan 0.93454545        nan        nan\n",
      " 0.97272727        nan        nan 0.94363636        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.96363636        nan        nan 0.95363636        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan\n",
      " 0.95454545        nan        nan 0.94454545        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "\n",
    "if dataset == 0:\n",
    "    n_components = np.arange(1, 9, 1)\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(2, 5, 1)\n",
    "\n",
    "param_grid = {\n",
    "    'lda__n_components': n_components,  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with LDA and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('lda', LDA()),  # LDA step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_lda = grid_search.best_params_\n",
    "best_accuracy_lda = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_lda}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_lda}\")\n",
    "\n",
    "best_model_lda = grid_search.best_estimator_\n",
    "test_accuracy_lda = best_model_lda.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_lda}\")\n",
    "\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_lda = compute_test(X_train, y_train, best_model_lda, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 14, 'knn__weights': 'uniform', 'svd__n_components': 4}\n",
      "Best Cross-Validation Accuracy: 0.9445454545454547\n",
      "Test Accuracy with Best Model: 1.0\n",
      "\n",
      "Performing Compute Test on the best model:\n",
      "Mean Cross-Validation Accuracy: 0.9236363636363636\n"
     ]
    }
   ],
   "source": [
    "if dataset == 0:\n",
    "    n_components = np.arange(2, 60, 2)\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(2, 5, 1)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'svd__n_components': np.arange(2, 5, 1),  # Number of dimensions to test\n",
    "    'knn__n_neighbors': np.arange(1, 21, 1),   # Number of neighbors to test\n",
    "    'knn__weights': ['uniform', 'distance'],  # Weighting schemes\n",
    "    'knn__metric': ['euclidean', 'manhattan']  # Distance metrics\n",
    "}\n",
    "\n",
    "# Create a pipeline with SVD and k-NN\n",
    "pipeline = Pipeline([\n",
    "    ('svd', TruncatedSVD()),  # SVD step\n",
    "    ('knn', KNeighborsClassifier())  # k-NN step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_svd = grid_search.best_params_\n",
    "best_accuracy_svd = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_svd}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_accuracy_svd}\")\n",
    "\n",
    "best_model_svd = grid_search.best_estimator_\n",
    "test_accuracy_svd = best_model_svd.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy with Best Model: {test_accuracy_svd}\")\n",
    "\n",
    "print(\"\\nPerforming Compute Test on the best model:\")\n",
    "result_svd = compute_test(X_train, y_train, best_model_svd, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Method: PCA\n",
      "Best Parameters for PCA: {'knn__metric': 'euclidean', 'knn__n_neighbors': 14, 'knn__weights': 'uniform', 'pca__n_components': 4}\n",
      "Best Cross-Validation Accuracy for PCA: 0.9445454545454547\n",
      "Test Accuracy for PCA: 1.0\n",
      "Cross-Validation Accuracy for PCA: 0.9236363636363636\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'PCA': {\n",
    "        'best_params': best_params_pca,\n",
    "        'best_accuracy': best_accuracy_pca,\n",
    "        'test_accuracy': test_accuracy_pca,\n",
    "        'cv_accuracy': result_pca\n",
    "    },\n",
    "    'SVD': {\n",
    "        'best_params': best_params_svd,\n",
    "        'best_accuracy': best_accuracy_svd,\n",
    "        'test_accuracy': test_accuracy_svd,\n",
    "        'cv_accuracy': result_svd\n",
    "    },\n",
    "    'LDA': {\n",
    "        'best_params': best_params_lda,\n",
    "        'best_accuracy': best_accuracy_lda,\n",
    "        'test_accuracy': test_accuracy_lda,\n",
    "        'cv_accuracy': result_lda\n",
    "    }\n",
    "}\n",
    "\n",
    "best_method = max(results, key=lambda x: results[x]['test_accuracy'])\n",
    "\n",
    "# Print out the best method and comparison results\n",
    "print(f\"\\nBest Method: {best_method}\")\n",
    "print(f\"Best Parameters for {best_method}: {results[best_method]['best_params']}\")\n",
    "print(f\"Best Cross-Validation Accuracy for {best_method}: {results[best_method]['best_accuracy']}\")\n",
    "print(f\"Test Accuracy for {best_method}: {results[best_method]['test_accuracy']}\")\n",
    "print(f\"Cross-Validation Accuracy for {best_method}: {results[best_method]['cv_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for MLP: {'mlp__hidden_layer_sizes': (150, 150), 'mlp__learning_rate_init': 0.01, 'mlp__max_iter': 100, 'pca__n_components': 4}\n",
      "Best Cross-Validation Accuracy for MLP: 0.9536363636363637\n",
      "Test Accuracy for MLP: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Accuracy: 0.9345454545454546\n",
      "Mean Cross-Validation Accuracy for MLP: 0.9345454545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if dataset == 0:\n",
    "    n_components = np.arange(2, 60, 2)\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(2, 5, 1)\n",
    "\n",
    "# Define parameter grid for MLP\n",
    "param_grid_mlp = {\n",
    "    'pca__n_components': n_components,  # Number of components to test for PCA\n",
    "    'mlp__hidden_layer_sizes': [(100, 100), (150, 150), (50, 50)],  # Hidden layer configurations\n",
    "    'mlp__learning_rate_init': [0.001, 0.01, 0.02],  # Learning rates\n",
    "    'mlp__max_iter': [10, 50, 100]  # Iterations for training\n",
    "}\n",
    "\n",
    "# Create a pipeline with PCA and MLPClassifier\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('mlp', MLPClassifier(activation='relu', solver='sgd', learning_rate='constant'))  # MLP step\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV for MLP\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    pipeline_mlp,\n",
    "    param_grid_mlp,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "best_params_mlp = grid_search_mlp.best_params_\n",
    "best_accuracy_mlp = grid_search_mlp.best_score_\n",
    "\n",
    "print(f\"Best Parameters for MLP: {best_params_mlp}\")\n",
    "print(f\"Best Cross-Validation Accuracy for MLP: {best_accuracy_mlp}\")\n",
    "\n",
    "best_model_mlp = grid_search_mlp.best_estimator_\n",
    "test_accuracy_mlp = best_model_mlp.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for MLP: {test_accuracy_mlp}\")\n",
    "\n",
    "result_mlp = compute_test(X_train, y_train, best_model_mlp, cv=10)\n",
    "print(f\"Mean Cross-Validation Accuracy for MLP: {result_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Bagging: {'bagging__estimator__max_depth': 5, 'bagging__n_estimators': 10, 'pca__n_components': 3}\n",
      "Best Cross-Validation Accuracy for Bagging: 0.9238095238095237\n",
      "Best Parameters for AdaBoost: {'adaboost__learning_rate': 0.01, 'adaboost__n_estimators': 50, 'pca__n_components': 2}\n",
      "Best Cross-Validation Accuracy for AdaBoost: 0.8952380952380953\n",
      "Best Parameters for Gradient Boosting: {'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'gb__n_estimators': 50, 'pca__n_components': 3}\n",
      "Best Cross-Validation Accuracy for Gradient Boosting: 0.9142857142857143\n",
      "Test Accuracy for Bagging: 0.9777777777777777\n",
      "Test Accuracy for AdaBoost: 0.9333333333333333\n",
      "Test Accuracy for Gradient Boosting: 0.9777777777777777\n",
      "Mean Cross-Validation Accuracy: 0.9142857142857143\n",
      "Mean Cross-Validation Accuracy: 0.8857142857142856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validation Accuracy: 0.9142857142857143\n",
      "Mean Cross-Validation Accuracy for Bagging: 0.9142857142857143\n",
      "Mean Cross-Validation Accuracy for AdaBoost: 0.8857142857142856\n",
      "Mean Cross-Validation Accuracy for Gradient Boosting: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "if dataset == 0:\n",
    "    n_components = [2, 10, 20, 30]\n",
    "elif dataset == 1:\n",
    "    n_components = np.arange(2, 5, 1)\n",
    "\n",
    "param_grid_bagging = {\n",
    "    'pca__n_components': n_components,  # PCA components to test\n",
    "    'bagging__n_estimators': [10, 50, 100],  # Number of base learners\n",
    "    'bagging__estimator__max_depth': [5, 10]  # Max depth for base estimator\n",
    "}\n",
    "\n",
    "# AdaBoost Classifier\n",
    "param_grid_adaboost = {\n",
    "    'pca__n_components': n_components,  # PCA components to test\n",
    "    'adaboost__n_estimators': [50, 100],  # Number of base learners\n",
    "    'adaboost__learning_rate': [0.01, 0.1]  # Learning rate for AdaBoost\n",
    "}\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "param_grid_gb = {\n",
    "    'pca__n_components': n_components,  # PCA components to test\n",
    "    'gb__n_estimators': [50, 100],  # Number of base learners\n",
    "    'gb__learning_rate': [0.01, 0.1],  # Learning rate for Gradient Boosting\n",
    "    'gb__max_depth': [3, 5]  # Max depth for trees in Gradient Boosting\n",
    "}\n",
    "\n",
    "pipeline_bagging = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('bagging', BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42))  # Bagging step\n",
    "])\n",
    "\n",
    "pipeline_adaboost = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('adaboost', AdaBoostClassifier(random_state=42))  # AdaBoost step\n",
    "])\n",
    "\n",
    "pipeline_gb = Pipeline([\n",
    "    ('pca', PCA()),  # PCA step\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))  # Gradient Boosting step\n",
    "])\n",
    "\n",
    "grid_search_bagging = GridSearchCV(pipeline_bagging, param_grid_bagging, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_adaboost = GridSearchCV(pipeline_adaboost, param_grid_adaboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search for each ensemble method\n",
    "grid_search_bagging.fit(X_train, y_train)\n",
    "grid_search_adaboost.fit(X_train, y_train)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best parameters and accuracy for Bagging\n",
    "best_params_bagging = grid_search_bagging.best_params_\n",
    "best_accuracy_bagging = grid_search_bagging.best_score_\n",
    "print(f\"Best Parameters for Bagging: {best_params_bagging}\")\n",
    "print(f\"Best Cross-Validation Accuracy for Bagging: {best_accuracy_bagging}\")\n",
    "\n",
    "# Retrieve best parameters and accuracy for AdaBoost\n",
    "best_params_adaboost = grid_search_adaboost.best_params_\n",
    "best_accuracy_adaboost = grid_search_adaboost.best_score_\n",
    "print(f\"Best Parameters for AdaBoost: {best_params_adaboost}\")\n",
    "print(f\"Best Cross-Validation Accuracy for AdaBoost: {best_accuracy_adaboost}\")\n",
    "\n",
    "# Retrieve best parameters and accuracy for Gradient Boosting\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "best_accuracy_gb = grid_search_gb.best_score_\n",
    "print(f\"Best Parameters for Gradient Boosting: {best_params_gb}\")\n",
    "print(f\"Best Cross-Validation Accuracy for Gradient Boosting: {best_accuracy_gb}\")\n",
    "\n",
    "# Evaluate the best model for Bagging on test data\n",
    "best_model_bagging = grid_search_bagging.best_estimator_\n",
    "test_accuracy_bagging = best_model_bagging.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for Bagging: {test_accuracy_bagging}\")\n",
    "\n",
    "# Evaluate the best model for AdaBoost on test data\n",
    "best_model_adaboost = grid_search_adaboost.best_estimator_\n",
    "test_accuracy_adaboost = best_model_adaboost.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for AdaBoost: {test_accuracy_adaboost}\")\n",
    "\n",
    "# Evaluate the best model for Gradient Boosting on test data\n",
    "best_model_gb = grid_search_gb.best_estimator_\n",
    "test_accuracy_gb = best_model_gb.score(X_test, y_test)\n",
    "print(f\"Test Accuracy for Gradient Boosting: {test_accuracy_gb}\")\n",
    "\n",
    "# Perform compute_test for each ensemble method\n",
    "result_bagging = compute_test(X_train, y_train, best_model_bagging, cv=5)\n",
    "result_adaboost = compute_test(X_train, y_train, best_model_adaboost, cv=5)\n",
    "result_gb = compute_test(X_train, y_train, best_model_gb, cv=5)\n",
    "\n",
    "print(f\"Mean Cross-Validation Accuracy for Bagging: {result_bagging}\")\n",
    "print(f\"Mean Cross-Validation Accuracy for AdaBoost: {result_adaboost}\")\n",
    "print(f\"Mean Cross-Validation Accuracy for Gradient Boosting: {result_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Ensemble Method: Bagging\n",
      "Best Parameters: {'bagging__estimator__max_depth': 5, 'bagging__n_estimators': 10, 'pca__n_components': 3}\n",
      "Test Accuracy: 0.9777777777777777\n",
      "Mean Cross-Validation Accuracy: 0.9142857142857143\n",
      "Mean Cross-Validation Accuracy for Best Model: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "# Store the best test accuracies and models\n",
    "best_model_info = {\n",
    "    \"Bagging\": {\"accuracy\": test_accuracy_bagging, \"model\": best_model_bagging, \"params\": best_params_bagging},\n",
    "    \"AdaBoost\": {\"accuracy\": test_accuracy_adaboost, \"model\": best_model_adaboost, \"params\": best_params_adaboost},\n",
    "    \"Gradient Boosting\": {\"accuracy\": test_accuracy_gb, \"model\": best_model_gb, \"params\": best_params_gb}\n",
    "}\n",
    "\n",
    "\n",
    "# Compare the test accuracies and print the best model\n",
    "best_model_name = max(best_model_info, key=lambda x: best_model_info[x][\"accuracy\"])\n",
    "best_model_accuracy = best_model_info[best_model_name][\"accuracy\"]\n",
    "best_model_params = best_model_info[best_model_name][\"params\"]\n",
    "\n",
    "print(f\"\\nBest Ensemble Method: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_model_params}\")\n",
    "print(f\"Test Accuracy: {best_model_accuracy}\")\n",
    "\n",
    "# Perform compute_test on the best model\n",
    "best_model = best_model_info[best_model_name][\"model\"]\n",
    "result_best_model = compute_test(X_train, y_train, best_model, cv=5)\n",
    "print(f\"Mean Cross-Validation Accuracy for Best Model: {result_best_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
